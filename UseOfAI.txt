Use Of AI
---------

[1]. Tool: Claude Code
Prompt: "I have two directories DAMAGE_DIR and NO_DAMAGE_DIR and I have two lists called damage_files and no_damage_files. Write Python code that samples 100 images from each class, collects their sizes, finds unique sizes, and prints a summary. When there are 5 or fewer unique sizes, print each size and how many images have that size"
Output:
# Check image dimension variability
print("\nChecking image dimension variability...")
damage_sizes = []
no_damage_sizes = []

# Sample 100 images from each class
for fname in damage_files[:100]:
    img = Image.open(os.path.join(DAMAGE_DIR, fname))
    damage_sizes.append(img.size)

for fname in no_damage_files[:100]:
    img = Image.open(os.path.join(NO_DAMAGE_DIR, fname))
    no_damage_sizes.append(img.size)

unique_sizes = set(damage_sizes + no_damage_sizes)
print(f"  Unique image sizes (from sample of 200): {len(unique_sizes)}")
if len(unique_sizes) <= 5:
    for size in unique_sizes:
        count = (damage_sizes + no_damage_sizes).count(size)
        print(f"    {size}: {count} images")


[2]. Tool: Claude Code
Prompt: "Create ImageDataGenerator configs for training and testing. The generator should rescale to 1/255 and have rotation up to 20 degrees, width and height shifts of 0.2, horizontal flip, zoom range of 0.2, and a validation split of 0.15"
Output:
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2,
    validation_split=0.15
)
test_datagen = ImageDataGenerator(rescale=1./255)

[3]. Tool: Claude Code
Prompt: "Write code that takes an image from DAMAGE_DIR, resizes it, and then display 10 different augmented versions of it? Save as as 'augmentation_samples.png'"
Output:
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

sample_img = Image.open(os.path.join(DAMAGE_DIR, damage_files[0]))
sample_img = sample_img.resize((IMG_HEIGHT, IMG_WIDTH))
img_array = np.array(sample_img)
img_array = img_array.reshape((1,) + img_array.shape)

# Generate augmented samples
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Data Augmentation Examples', fontsize=16)

i = 0
for batch in datagen.flow(img_array, batch_size=1):
    ax = axes[i // 5, i % 5]
    ax.imshow(batch[0].astype('uint8'))
    ax.axis('off')
    i += 1
    if i >= 10:
        break

plt.tight_layout()
plt.savefig('augmentation_samples.png', dpi=150, bbox_inches='tight')
plt.show()

[4]. Tool: Claude Code
Prompt: "Create EarlyStopping for val_loss (patience 5) and ModelCheckpoint to save best model based on val_accuracy for the dense-ann model"
Output:
# The code below was generated by AI; see [4].
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

checkpoint = ModelCheckpoint(
    'dense_ann_best.keras',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

[5]. Tool: Claude Code
Prompt: "Create EarlyStopping for val_loss (patience 5) and ModelCheckpoint to save best model based on val_accuracy for the lenet5-model"
Output:
# Callbacks
early_stopping_lenet5 = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

checkpoint_lenet5 = ModelCheckpoint(
    'lenet5_best.keras',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

[6]. Tool: Claude Code
Prompt: "Create EarlyStopping for val_loss (patience 5) and ModelCheckpoint to save best model based on val_accuracy for the alternate_lenet5 model" 
Output:
early_stopping_alt = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

checkpoint_alt = ModelCheckpoint(
    'alternate_lenet5_best.keras',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

[7]. Tool: Claude Code
Prompt: "Create a dictionary comparing three models with their test accuracy and loss. Print a comparison table and determine the best model based on accuracy"
Output:
results = {
    'Dense ANN': {'accuracy': test_accuracy_dense, 'loss': test_loss_dense},
    'LeNet-5': {'accuracy': test_accuracy_lenet5, 'loss': test_loss_lenet5},
    'Alternate LeNet-5': {'accuracy': test_accuracy_alt, 'loss': test_loss_alt}
}

print("=" * 60)
print("FINAL MODEL COMPARISON")
print("=" * 60)
for model_name, metrics in results.items():
    print(f"{model_name:20s} - Accuracy: {metrics['accuracy']:.4f}, Loss: {metrics['loss']:.4f}")

# Determine best model
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
print(f"\nBest Model: {best_model_name} with accuracy {results[best_model_name]['accuracy']:.4f}")

[8]. Tool: Cursor (ChatGPT 5.1)
Prompt: "Make a Dockerfile for my Flask inference server with Python 3.9 slim. Install flask, pillow, tensorflow, and numpy. Copy inference_server.py and the artifacts folder. Run the server on Port 5000"
Output: DockerFile

[9]. Tool: Cursor (ChatGPT 5.1)
Prompt: "Make a comprehensive README for the Docker inference server. Include 'instructions for using the container image, docker-compose file and example requests'"
Output: README.md


